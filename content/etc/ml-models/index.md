---
title: 'Machine learning models'
date: 2024-07-24T18:31:26-07:00
draft: false
author: "Monica Spisar"
authorLink: "https://monicaspisar.com"
description: 'Resources: Machine learning models'
images: 
- "/posts/.../....png"
tags: []
categories: []
resources:
- name: "foo"
  src: "foo.png"
math:
  enable: true
---

## Techniques
### Transformers
[The Illustrated Transformer – Jay Alammar: Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/)

### Random initialization
[Randomly Initialized One-Layer Neural Networks Make Data Linearly Separable](https://arxiv.org/abs/2205.11716)

## Model components
### Loss functions
[Loss Functions and Metrics in Deep Learning](https://arxiv.org/abs/2307.02694)

[Choice of loss function matters: Inside the maths that drives AI](https://www.nature.com/articles/d41586-024-02185-z)
- [Choice of loss function matters: Inside the maths that drives AI (pdf)](https://media.nature.com/original/magazine-assets/d41586-024-02185-z/d41586-024-02185-z.pdf)

## Interpretability
### Experiments in reverse-engineering models
[OthelloGPT learned a bag of heuristics — LessWrong](https://www.lesswrong.com/posts/gcpNuEZnxAPayaKBY/othellogpt-learned-a-bag-of-heuristics-1)

## Practice projects
### Karpathy: Micrograd
[karpathy/micrograd: A tiny scalar\-valued autograd engine and a neural net library on top of it with PyTorch-like API (github)](https://github.com/karpathy/micrograd)

[The spelled\-out intro to neural networks and backpropagation: building micrograd (YouTube)](https://www.youtube.com/watch?v=VMj-3S1tku0)


### Karpathy: [Neural networks: zero to hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
[Implement GPT2](https://www.youtube.com/watch?v=l8pRSuU81PU)