---
title: 'Machine learning fundamentals'
date: 2024-07-21T16:37:09-07:00
draft: false
author: "Monica Spisar"
authorLink: "https://monicaspisar.com"
description: 'Machine learning fundamentals'
images: 
- "/posts/.../....png"
tags: []
categories: []
resources:
- name: "foo"
  src: "foo.png"
math:
  enable: true
---
#### &#127793; draft &#127793;

## Where to begin?
Just starting out in machine learning? Know your way around but want to dig in deeper? Either way, [Vicky Boykis](https://vickiboykis.com/) has a terrific resource for you.

Her [Anti-hype LLM reading list gist](https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e) starts with a timeline sketch of 1990s statistical learning through today's LLMs, then provides curated resources for topics including LLM building blocks, deep learning, transformers/attention, GPT, open source models, training data, pre-training, RLHF and DPO, fine-tuning and compression, small LLMs, GPUs, evaluation, and UX. 

It's a fine place to get a lay of the land and then get cosy with ML fundamentals.

- [Vicky Boykis' Anti-hype LLM reading list](https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e)

## Academic courses
Some well-known machine learning courses offered at universities such as Stanford, CMU, Harvard, MIT, etc, post their materials. Links to just a few of those:

- [Stanford CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/ "CS231n Convolutional Neural Networks for Visual Recognition")

- [MIT 6.790 Machine Learning (grad)](https://gradml.mit.edu/info/calendar/ "Calendar | 6.790 Machine Learning")

- [Harvard CS181 - syllabus](https://harvard-ml-courses.github.io/cs181-web/syllabus "Syllabus | CS181")

  - [Harvard CS181 - textbook/notes](https://github.com/harvard-ml-courses/cs181-textbook "harvard-ml-courses/cs181-textbook")

- [CMU Machine Learning 10-601: Lectures](https://www.cs.cmu.edu/~ninamf/courses/601sp15/lectures.shtml "Machine Learning 10-601: Lectures")

---

## Neural networks
This is an extremely non-comprehensive collection of links to either foundational papers or detailed explanations of foundational concepts. 

### Embeddings
[What are embeddings\?](https://vickiboykis.com/what_are_embeddings/)

### Backprop
[The paper that started it all: Letters to Nature: Learning representation by back-propagating errors (pdf)](https://www.cs.toronto.edu/~hinton/absps/naturebp.pdf)

### Convolutional neural nets
[ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)

### Transformers
[A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)

### Training
[A Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/)
