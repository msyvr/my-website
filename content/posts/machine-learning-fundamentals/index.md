---
title: 'Machine learning fundamentals'
date: 2024-07-21T16:37:09-07:00
draft: false
author: "Monica Spisar"
authorLink: "https://monicaspisar.com"
description: 'Machine learning fundamentals'
images: 
- "/posts/.../....png"
tags: []
categories: []
resources:
- name: "foo"
  src: "foo.png"
math:
  enable: true
---

## Backprop
[The paper that started it all: Letters to Nature: Learning representation by back-propagating errors (pdf)](https://www.cs.toronto.edu/~hinton/absps/naturebp.pdf)

## Batch norm
[How does batch normalization help optimization? (pdf)](https://arxiv.org/pdf/1805.11604)

## Model evaluation
[Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)

## Fine tuning models: domain-adaptive pre-training

[Continual Pre-training of Language Models](https://arxiv.org/abs/2302.03241)

## Synthetic data
[Machine Learning for Synthetic Data Generation: A Review](https://arxiv.org/abs/2302.04062)

[Cosmopedia\: how to create large\-scale synthetic data for pre\-training Large Language Models](https://huggingface.co/blog/cosmopedia "Cosmopedia: how to create large-scale synthetic data for pre-training Large Language Models")

## Practice projects
### Karpathy: Micrograd
[karpathy/micrograd: A tiny scalar\-valued autograd engine and a neural net library on top of it with PyTorch-like API (github)](https://github.com/karpathy/micrograd)

[The spelled\-out intro to neural networks and backpropagation: building micrograd (YouTube)](https://www.youtube.com/watch?v=VMj-3S1tku0)

[Karpathy on Micrograd: "These 94 lines of code are everything that is needed to train a neural network. Everything else is just efficiency."](https://x.com/karpathy/status/1803963383018066272)

### Karpathy: [Neural networks: zero to hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
[Implement GPT2](https://www.youtube.com/watch?v=l8pRSuU81PU)